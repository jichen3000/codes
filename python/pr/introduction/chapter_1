chapter 1. Introduction

reinforcement learning
trial and error.
probability theory, decision theory, and infor- mation theory. 

polynomial function p24 1.1
coefficients 系数
error function p24 1.2
model comparison or model selection.
M is the order of the polynomial,
over-fitting.

RMS errors
training dataset的数量应该是M的5-10倍，这样可以避免over fitting的问题

By adopting a Bayesian approach, the over-fitting problem can be avoided. 

One technique that is often used to control the over-fitting phenomenon in such cases is that of regularization, which involves adding a penalty term to the error function (1.2) in order to discourage the coefficients from reaching large values.

The particular case of a quadratic regularizer is called ridge regres- sion (Hoerl and Kennard, 1970). 
In the context of neural networks, this approach is known as weight decay.

probability.
sum rule p14
product rule

marginal probability,
conditional probability
product rule
Bayes’ theorem p15
denominator 分母
numerator 分子

probability density p17/36

cumulative distribution function

measure theory

The average value of some function f(x) under a probability distribution p(x) is called the expectation of f (x) and will be denoted by E[f]. p19/38

likelihood function.
p(D|w)

posterior = likehood * prior

In the machine learning literature, the negative log of the likelihood function is called an error function.

在概率论中，概率质量函数（probability mass function，简写为pmf）是离散随机变量在各特定取值上的概率。概率质量函数和概率密度函数不同之处在于：概率质量函数是对离散随机变量定义的，本身代表该值的概率；概率密度函数是对连续随机变量定义的，本身不是概率，只有对连续随机变量的概率密度函数在某区间内进行积分后才是概率。

normal or Gaussian distribution. p43 1.46

μ, called the mean, and σ2, called the vari- ance. The square root of the variance, given by σ, is called the standard deviation, and the reciprocal of the variance, written as β = 1/σ2, is called the precision.

independent and identically distributed, which is often abbreviated to i.i.d 