Improving classification with the AdaBoost meta-algorithm

Meta-algorithms are a way of combining other algorithms.

AdaBoost
Pros: Low generalization error, easy to code, works with most classifiers, no parameters to adjust
Cons: Sensitive to outliers
Works with: Numeric values, nominal values

One idea that naturally arises is combining multiple classifiers. Methods that do this are known as ensemble methods or meta-algorithms.
Methods:
    bagging
    boosting
        AdaBoost

AdaBoost is short for adaptive boosting.

A decision stump is a simple decision tree.

The classifying result is better and faster than logistic regression.

But when you using too much times boosting, it will be overfitting.

AdaBoost and support vector machines are considered by many to be the most power- ful algorithms in supervised learning.

Classification imbalance:
Precision = TP/(TP+FP).
Precision tells us the fraction of records that were positive from the group that the classifier predicted to be positive.
Recall = TP/(TP+FN). Recall measures the fraction of positive examples the classifier got right.
ROC curve. ROC stands for receiver operating characteristic

cost function
